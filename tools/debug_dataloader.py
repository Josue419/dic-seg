"""
æ•°æ®åŠ è½½å™¨è°ƒè¯•å·¥å…· - ä¿®å¤ç‰ˆæœ¬

ä¿®å¤é—®é¢˜ï¼š
1. æ­£ç¡®å¯¼å…¥æ‰€æœ‰ mmseg transforms
2. å¢å¼ºé”™è¯¯å¤„ç†å’Œè°ƒè¯•ä¿¡æ¯
3. é€æ­¥éªŒè¯ pipeline çš„æ¯ä¸ªæ­¥éª¤
"""

import argparse
import sys
from pathlib import Path
import torch
from mmengine.config import Config
from mmengine.dataset import DefaultSampler
from torch.utils.data import DataLoader
from mmengine.dataset import default_collate
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
sys.path.insert(0, str(Path(__file__).resolve().parents[1]))

# âœ… å…³é”®ä¿®å¤ï¼šæ­£ç¡®å¯¼å…¥æ‰€æœ‰å¿…éœ€æ¨¡å—
import mmseg
import mmseg.datasets
import mmseg.models
import mmseg.datasets.transforms  # âœ… ç¡®ä¿ PackSegInputs è¢«æ³¨å†Œ

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from mmseg_custom import *


def debug_single_sample(dataset, idx=0):
    """è°ƒè¯•å•ä¸ªæ ·æœ¬çš„æ•°æ®ç»“æ„"""
    print(f"\n{'='*60}")
    print(f"[DEBUG] åˆ†ææ•°æ®é›†æ ·æœ¬ #{idx}")
    print(f"{'='*60}")
    
    try:
        # è·å–åŸå§‹æ•°æ®ä¿¡æ¯
        data_info = dataset.get_data_info(idx)
        print(f"âœ“ åŸå§‹æ•°æ®ä¿¡æ¯:")
        for key, value in data_info.items():
            if isinstance(value, str) and len(value) > 50:
                print(f"  {key}: {value[:50]}...")
            else:
                print(f"  {key}: {value}")
        
        # æ£€æŸ¥å¿…éœ€çš„é”®
        required_keys = ['img_path', 'seg_map_path', 'seg_fields']
        for key in required_keys:
            if key not in data_info:
                print(f"  âŒ ç¼ºå°‘å¿…éœ€é”®: {key}")
                return False
            else:
                print(f"  âœ… åŒ…å«å¿…éœ€é”®: {key}")
        
        # é€šè¿‡ pipeline å¤„ç†
        print(f"\nğŸ“¥ é€šè¿‡ pipeline å¤„ç†...")
        processed_data = dataset[idx]
        
        print(f"âœ“ å¤„ç†åçš„æ•°æ®ç»“æ„:")
        for key, value in processed_data.items():
            if hasattr(value, 'shape'):
                print(f"  {key}: {type(value)} - shape: {value.shape}")
            elif hasattr(value, '__len__') and not isinstance(value, str):
                print(f"  {key}: {type(value)} - length: {len(value)}")
            else:
                print(f"  {key}: {type(value)}")
        
        # âœ… å…³é”®æ£€æŸ¥ï¼šinputs å¿…é¡»æ˜¯ tensor
        if 'inputs' in processed_data:
            inputs = processed_data['inputs']
            print(f"\nğŸ” å…³é”®æ£€æŸ¥ - inputs:")
            print(f"  ç±»å‹: {type(inputs)}")
            if torch.is_tensor(inputs):
                print(f"  âœ… æ˜¯ torch.Tensor!")
                print(f"  å½¢çŠ¶: {inputs.shape}")
                print(f"  æ•°æ®ç±»å‹: {inputs.dtype}")
                print(f"  è®¾å¤‡: {inputs.device}")
            else:
                print(f"  âŒ ä¸æ˜¯ tensor! å®é™…ç±»å‹: {type(inputs)}")
                if isinstance(inputs, list):
                    print(f"  åˆ—è¡¨é•¿åº¦: {len(inputs)}")
                    if inputs:
                        print(f"  ç¬¬ä¸€ä¸ªå…ƒç´ ç±»å‹: {type(inputs[0])}")
                return False
        
        # æ£€æŸ¥ data_samples
        if 'data_samples' in processed_data:
            data_samples = processed_data['data_samples']
            print(f"\nğŸ” å…³é”®æ£€æŸ¥ - data_samples:")
            print(f"  ç±»å‹: {type(data_samples)}")
            
            # æ£€æŸ¥å¤©æ°”æ ‡ç­¾
            weather_label = None
            if hasattr(data_samples, 'metainfo') and data_samples.metainfo:
                weather_label = data_samples.metainfo.get('weather_label')
                print(f"  âœ… å¤©æ°”æ ‡ç­¾: {weather_label}")
            else:
                print(f"  âš  æ²¡æœ‰æ‰¾åˆ°å¤©æ°”æ ‡ç­¾")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ ·æœ¬ #{idx} å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def debug_pipeline_steps(cfg):
    """é€æ­¥è°ƒè¯• pipeline ä¸­çš„æ¯ä¸ª transform"""
    print(f"\n{'='*60}")
    print(f"[DEBUG] é€æ­¥è°ƒè¯• Pipeline")
    print(f"{'='*60}")
    
    try:
        # æ„å»ºæ•°æ®é›†ï¼ˆä¸ä½¿ç”¨ pipelineï¼‰
        train_dataset_cfg = cfg.train_dataloader.dataset.copy()
        train_dataset_cfg['pipeline'] = []  # æš‚æ—¶æ¸…ç©º pipeline
        
        from mmseg.registry import DATASETS
        dataset = DATASETS.build(train_dataset_cfg)
        
        # è·å–åŸå§‹æ•°æ®
        data_info = dataset.get_data_info(0)
        print(f"âœ“ åŸå§‹æ•°æ®ä¿¡æ¯: {list(data_info.keys())}")
        
        # æ£€æŸ¥å¿…éœ€é”®
        if 'seg_fields' not in data_info:
            print(f"âŒ åŸå§‹æ•°æ®ç¼ºå°‘ seg_fields é”®!")
            return False
        
        # é€æ­¥åº”ç”¨æ¯ä¸ª transform
        pipeline = cfg.train_dataloader.dataset.pipeline
        current_data = data_info.copy()
        
        from mmseg.registry import TRANSFORMS
        
        for step_idx, transform_cfg in enumerate(pipeline):
            transform_type = transform_cfg['type']
            print(f"\nğŸ”„ æ­¥éª¤ {step_idx + 1}: {transform_type}")
            
            try:
                # æ„å»º transform
                transform = TRANSFORMS.build(transform_cfg)
                print(f"  âœ… Transform æ„å»ºæˆåŠŸ: {type(transform)}")
                
                # åº”ç”¨ transform
                current_data = transform(current_data)
                
                print(f"  âœ… Transform åº”ç”¨æˆåŠŸ")
                print(f"  è¾“å‡ºé”®: {list(current_data.keys())}")
                
                # æ£€æŸ¥å…³é”®å­—æ®µ
                for key in ['inputs', 'data_samples', 'img', 'gt_seg_map']:
                    if key in current_data:
                        value = current_data[key]
                        if hasattr(value, 'shape'):
                            print(f"    {key}: {type(value)} - shape: {value.shape}")
                        else:
                            print(f"    {key}: {type(value)}")
                
                # ç‰¹åˆ«æ£€æŸ¥ inputs ç±»å‹
                if 'inputs' in current_data:
                    inputs = current_data['inputs']
                    if torch.is_tensor(inputs):
                        print(f"    âœ… inputs æ˜¯ torch.Tensor")
                    else:
                        print(f"    âŒ inputs ä¸æ˜¯ tensor: {type(inputs)}")
                
            except Exception as e:
                print(f"  âŒ å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return False
        
        print(f"\nâœ… Pipeline å…¨éƒ¨æ­¥éª¤æ‰§è¡ŒæˆåŠŸ!")
        return True
        
    except Exception as e:
        print(f"âŒ Pipeline è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def debug_dataloader(cfg):
    """è°ƒè¯•å®Œæ•´çš„ DataLoader"""
    print(f"\n{'='*60}")
    print(f"[DEBUG] åˆ†æ DataLoader")
    print(f"{'='*60}")
    
    try:
        # æ„å»ºæ•°æ®é›†
        train_dataset_cfg = cfg.train_dataloader.dataset.copy()
        
        from mmseg.registry import DATASETS
        dataset = DATASETS.build(train_dataset_cfg)
        
        print(f"âœ“ æ•°æ®é›†æ„å»ºæˆåŠŸ: {len(dataset)} æ ·æœ¬")
        
        # è°ƒè¯•å•ä¸ªæ ·æœ¬
        if not debug_single_sample(dataset, idx=0):
            return False
        
        # æ„å»º DataLoader
        sampler = DefaultSampler(dataset, shuffle=False)
        dataloader = DataLoader(
            dataset=dataset,
            batch_size=1,
            sampler=sampler,
            num_workers=0,
        
            collate_fn=default_collate,
        )
        
        print(f"\nğŸ“¦ DataLoader æ„å»ºæˆåŠŸ")
        
        # è·å–ç¬¬ä¸€ä¸ª batch
        print(f"\nğŸ”„ è·å–ç¬¬ä¸€ä¸ª batch...")
        for batch_idx, batch_data in enumerate(dataloader):
            print(f"âœ“ Batch #{batch_idx} è·å–æˆåŠŸ")
            print(f"  Batch ç±»å‹: {type(batch_data)}")
            
            # æ£€æŸ¥ inputs
            if 'inputs' in batch_data:
                inputs = batch_data['inputs']
                print(f"\nğŸ¯ å…³é”®æ£€æŸ¥ - batch inputs:")
                print(f"    ç±»å‹: {type(inputs)}")
                if torch.is_tensor(inputs):
                    print(f"    âœ… æ˜¯ tensor!")
                    print(f"    å½¢çŠ¶: {inputs.shape}")
                else:
                    print(f"    âŒ ä¸æ˜¯ tensor!")
                    return False
            
            # åªæµ‹è¯•ç¬¬ä¸€ä¸ª batch
            break
        
        return True
        
    except Exception as e:
        print(f"âŒ DataLoader è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    parser = argparse.ArgumentParser(description='Debug data loading issues')
    parser.add_argument('config', help='Config file path')
    args = parser.parse_args()
    
    print(f"\n{'='*80}")
    print(f"æ•°æ®åŠ è½½å™¨è°ƒè¯•å·¥å…· - ä¿®å¤ç‰ˆ")
    print(f"{'='*80}")
    print(f"é…ç½®æ–‡ä»¶: {args.config}")
    
    # åŠ è½½é…ç½®
    try:
        cfg = Config.fromfile(args.config)
        print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        return False
    
    # è°ƒè¯•æ­¥éª¤
    success = True
    
    # 1. é€æ­¥è°ƒè¯• pipeline
    print(f"\nğŸ“‹ æ­¥éª¤ 1: é€æ­¥è°ƒè¯• Pipeline")
    if not debug_pipeline_steps(cfg):
        success = False
    
    # 2. è°ƒè¯•å®Œæ•´ DataLoader
    if success:
        print(f"\nğŸ“¦ æ­¥éª¤ 2: è°ƒè¯•å®Œæ•´ DataLoader")
        if not debug_dataloader(cfg):
            success = False
    
    # æ€»ç»“
    print(f"\n{'='*80}")
    if success:
        print(f"âœ… æ‰€æœ‰è°ƒè¯•æ­¥éª¤é€šè¿‡!")
        print(f"æ•°æ®åŠ è½½ç®¡é“å·¥ä½œæ­£å¸¸ï¼Œinputs æ­£ç¡®è½¬æ¢ä¸º torch.Tensor")
    else:
        print(f"âŒ å‘ç°é—®é¢˜!")
        print(f"è¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤")
    print(f"{'='*80}")
    
    return success


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)